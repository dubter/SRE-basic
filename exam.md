# SRE. Вопросы к экзамену. Первое полугодие

## 1. Что такое SRE? Сферы ответственности SRE.
SRE — набор принципов и практик, которые нацелены на повышение качества и надежности эксплуатации систем.

Снятие телеметрии метрики типа, Алертинг, Устранение сбоев (~~ЛИКВИДИРОВАН~~)
Снятие телеметрии
- Помощь командам в дизайне и реализации мониторинга сервисов
- Дизайн и реализация мониторинга доступности
- Журналирование важных событий
Алертинг
- Выстраивание процессов доставки и реагирования на алерты
- Анализ качества реагирование на алерты
Устранение сбоев
- Реагирование на алерты в 3:00 и восстановление работоспособности системы
- Уведомление о сбоях
- Коордиация работ по устранению сбоев
Анализ сбоев
- Поиск путей улучшения сервисов на основании прошлых сбоев
- Бюджетирование простоев (бюджет ошибок)
- Сбор статистики по сбоям
Обучение
- Обучение разработчиков приемам по улучшению отказоустойчивости сервисов
- Консультирование команд
- Обучение новых членов команд
Доставка новых версий приложения
- Внедрение безопаных подходов к доставке приложений
- Тестирование новых версий в продакшен
DRP / DiRT __(Disaster Recovery Plan / Disaster Recovery Testing)__
- План восстановления после катастроф/Тестирование восстановления после катастроф
- Симуляция отказов
- Учения по восстановлению

Архитектура
- Изучение отказоустойчивых архитектур
- Консультирование команд разработки
- Обучение коллег

Обеспечение вычислительными ресурсами
Тестирование
Инструменты для выявления причин сбоев
Конфигурирование приложений
Общие приемы отказоустойчивости
Публикация сервисов
Документирование

## 2. Сравните SRE и DevOps. Что такое необвинительная культура и зачем она?
SRE — инженерная специальность. Специалисты занимаются проектированием, наладкой, устранением неполадок, эксплуатацией, выведением из эксплуатации, утилизацией.

Эффективнее работать единой командой, потому что улучшается коммуникация. Принцип Amazon: You build it — you run it.

DevOps-инженеры, в отличие от SRE, не смотрят архитектуру, не должны уметь писать код, не так работают с надежностью, не должны разбираться в причинах сбоев. Обычно они работают с конфигурацией приложений, настраивают CI/CD-процессы и выступают первой линией ИТ-реагирования. 
SRE более опытны, чем DevOps, в плане разработки ПО. Хорошо, когда команда SRE содержит одного-двух опытных разработчиков. 


*Необвинительная культура*

Само наличие человеческого фактора важно принимать как нормальную ситуацию. Важно искать более глубокие причины и при этом внедрять необвинительную культуру — не ругать сотрудников. 
В работе с надежностью очень важна необвинительная культура, когда за ошибки людей не ругают и не штрафуют.
В противном случае люди будут скрывать ошибки и общая надежность систем будет падать.
Более выгодно сделать так, чтобы люди спокойно работали над ошибками.
Пример: в Google ошибки открыто обсуждаются, разбираются их причины, называются имена тех, кто их совершил.
Позиция компании: если человек получил образование, прошел собеседование, получил работу и все равно сделал ошибку,
которая привела к сбою, то с ним все в порядке, но что-то не так в нашей системе.
При этом нужно различать злой умысел и ошибки.


## 3. Зачем нужен мониторинг? Что такое метрики? Какие бывают типы метрик?
Мониторинг — это комплекс мероприятий, которые направлены на: 
Детектирование сбоев. Важно отделить периоды, когда системы работали исправно, от периодов, когда они не работали. Здесь важен критерий существенности.
Поиск причин сбоев при устранении.
Сбор информации для отладки. Это помогает разработчикам понять, что происходит с приложением в нормальном состоянии, и определить, на что тратятся ресурсы.
Выявление трендов. Важно, например, сравнивать приложение в прошлом и в настоящем. Примеры: как постепенно меняется нагрузка со стороны пользователей, замедляется ли работа приложения при добавлении новых функций. Поскольку тренды могут быть очень медленными, важно хранить информацию длительное время.

Метрики — серии числовых значений, привязанных к определенному времени. Обработка происходит через равные интервалы времени. Стандартный вариант — одна минута. Пример: отслеживание количества пространства на жестком диске в байтах.
Именование по группам (доменные имена): имя_дц.приложение.категория...
Именование по тегам (суть данной метрики):

Типы:
Counter — числовой ряд, который постоянно увеличивается. Если значения уменьшаются, значит, произошел рестарт приложения. Увеличивающиеся метрики удобно применять в расчетах.
Пример: вы хотите понять, сколько запросов обрабатывает приложение. Собирая данные раз в минуту, вы будете получать показатель прироста.
Gauge — метрики, меняющие значения произвольно. Используются, например, чтобы определить, сколько есть свободной оперативной памяти или как меняется температура процессора.
Частота сбора такой метрики должна быть ниже частоты изменения показателя, который вы изучаете.
Гистограммы — используются, например, для сбора данных о времени выполнения запросов.
- Первый вариант гистограммы — buckets. Мы создаем шкалу типичных значений, например 5, 10, 15 миллисекунд, и определяем, сколько запросов было обработано в пределах этих промежутков времени.
- Второй вариант гистограммы — quantiles. Обычно считают медиану, 95-й перцентиль и 99-й. Сложность — нужно накопить большой объем информации. Обычно это доступно в библиотеках для разных языков программирования. Важно понимать, каков смысл показателей, которые заложены в библиотеку.

## 4. Как и где собирают, хранят и обрабатывают метрики? Что делать если значение, которое мы хотим мониторить очень быстро меняется, например количество используемых соединений в пуле?

Для сбора информации метрик используются две модели: Push (заталкивание) и Pull (вытягивание). 

Push — приложение подключается к системе и записывает значения с указанным интервалом. Система их сохраняет, посылает запросы и анализирует значения. Интервал настраивается в нашем приложении. Приложение может перегрузить сбор метрик, если все метрики собираются, например, в нулевую секунду каждой минуты. Нагрузку нужно распределять равномерно в течение минуты.
Pull — сейчас более популярная модель. Сборщик — отдельно стоящее приложение — с указанным интервалом ходит по нашему приложению, делает http-вызов и получает значения. Если компания большая, то нужна целая «ферма». При неправильной настройке сборщики высокочастотных метрик могут перегрузить приложение запросами.

Программы для хранения и обработки метрик: 
- Prometheus — содержит в себе сборщик метрик и специальный язык запросов;
- Graphite;
- Victoria Metrics — частично заменяет Prometheus и поддерживает тот же язык запросов, но не умеет собирать метрики, в нее надо писать. Для этого часто используется Prometheus;
- InfluxDB;
- Zabbix;
- Nagios;
- Datadog.

## 5. Что такое кардинальность метрик? Какую она несет опасность?

> Это уникальность значений, и их опасность, если неправильно выбрать ключ, то метрик будет много.

Кардинальность метрик - это понятие, относящееся к количеству уникальных значений, которые может принимать метрика в системе мониторинга. Высокая кардинальность возникает, когда метрика имеет большое число различных значений, что может привести к ряду проблем:
1) Увеличение нагрузки на систему мониторинга: Высокая кардинальность требует больше ресурсов для хранения, обработки и анализа данных. Это может привести к снижению производительности системы мониторинга и увеличению затрат на инфраструктуру.
2) Замедление запросов и визуализаций: Системе мониторинга требуется больше времени для обработки запросов и построения графиков при высокой кардинальности. Это может сделать систему менее отзывчивой и затруднить анализ данных в режиме реального времени.
3) Сложность анализа данных: Большое количество уникальных значений метрики усложняет обнаружение закономерностей, аномалий и построение эффективных алертов. Это может привести к пропуску важных событий и затруднению принятия своевременных мер по устранению проблем.
4) Увеличение расходов на хранение: Высокая кардинальность требует больше дискового пространства для хранения данных метрик. Это может привести к значительному росту затрат, особенно при длительном хранении исторических данных.



## 6. Что такое логи? В чем отличие структурированных и неструктурированных логов?
 Лог — метка на временной шкале, снабженная произвольной информацией о том, что за событие произошло, почему произошло.

неструктурированных пожилой для чтения и анализа человеком
структурированных индастри стандарт и анализ машиной

## 7. Как и где собирают, хранят и обрабатывают логи? Как логгирование влияет на потребление ресурсов? Какие ошибки в организации логгирования можно допустить?


Хранение логов:
в файлах на сервере;
Systemd Journal;
централизованные агрегаторы — ElasticSearch, Grafana Loki, Splunk.

Как логи попадают в централизованные системы: 
Запись на диск, затем в агрегатор логов. В процессе передачи с диска в агрегатор возможно обогащение логов.
Запись напрямую в агрегатор логов. Преимущества — не задействован диск, не нужно мониторить процесс передачи. Недостаток — нужно подстраховаться на случай, если агрегатор недоступен при сбое. Можно копить логи, используя локальные диски. Это усложняет написание приложений.

На процессинг логов уходит на удивление много ресурсов. Например, приходит запрос по API. Он несложно обрабатывается, но при этом делается несколько записей в логи, по длинной цепочке отправляет в систему процессинга логов. Мы стремимся делать максимально легковесным процессинг логов. Преимущество — приложение делает готовый лог. 

Во время сбоев приложения пишут во много раз больше логов, чем в обычное время. Системы могут не всё писать об успешных транзакциях, но об ошибках пишут абсолютно всё. Растет и количество записей, и их размер. То есть система, которая должна помочь разобраться со сбоем, может сама не справиться с нагрузкой. В итоге на основной сбой накладывается сбой мониторинга. Здесь требуется грамотное проектирование.

## 8. Что такое семплирование логов? Какие проблемы решает? Что надо учесть при реализации семплирования логов?

Логи можно сэмплировать, то есть логировать не каждый запрос, а каждый десятый, сотый или тысячный в зависимости от нагрузки. Обычно 10% запросов, а иногда и 1% вполне достаточно для того, чтобы сделать выводы.

## 9. Для чего следует использовать логгирование, а для чего метрики? В чем разница этих двух методов снятия телеметрии? Назовите преимущества и недостатки обоих.

Основные проблемы метрик: 
Невозможно постфактум путем пересчетов получить то, что вы изначально не закладывали в метрики. Этим метрики отличаются от логов, в которых можно хранить информацию о каждом запросе.
Измеряемых метрик становится слишком много. В каждой организации много сервисов, каждый из которых хочет выставлять множество метрик. Большое количество метрик может перегрузить централизованную систему мониторинга. Приходится вводить лимиты и придумывать системы сдерживания мониторинга. Однако не все системы обработки метрик позволяют ввести лимиты и сделать это безопасно. Очень распространенная проблема — общая система мониторинга убивается одним из его участников. 


## 10. Что такое распределенная трассировка запросов? Какие задачи решает? Как реализуется?

Трассировка запроса
Сложные системы обычно состоят из нескольких тесно взаимодействующих подсистем. Конкретный запрос от пользователя проходит через массу микросервисов, которые делают согласованную работу. Поэтому удобно собирать логирование со всех сервисов. Если все запросы имеют сквозной идентификатор, по нему можно собрать полный жизненный цикл конкретного запроса. Такой прием называется трассировкой запроса.
Существуют специализированные системы для трассировки запроса. Проекты OpenTracing и OpenTelemetry работают над стандартизацией этой процедуры, пытаются провести ее через все языки программирования.
Реализация бэкендов: 
Jaeger;
Zipkin;
ElasticSearch.
Интерфейс отображает прохождение запроса через системы. Это очень помогает в отладке. Такие системы логируют выборку запросов или держат все запросы недолгое время, а для дальнейшего хранения делают выборку.


## 11. Как мы детектируем сбой на наших системах? Опишите прямые способы детектирования сбоев.
> Прямые методы. Мы тестируем функциональность приложения вручную, автоматически или иными способами.

Прямые методы — более точные.
- ручное тестирование. Самый простой вариант, применяется для подтверждения подозрения о сбое.
- Автотесты можно делать на продакшене непрерывно. Важно, чтобы они не мешали пользователям системы.
 Это наиболее быстрый и точный способ детектировать сбои. Они очень редко дают ложноположительный результат. Проблема таких тестов — дороговизна. Пример проберы — приложения, которые проводят автотесты.

 мы пишем специальные логи heartbeats

## 12. Как мы детектируем сбои? Опишите косвенные методы детектирования сбоев.
> Мы видим аномальные значения в показателях, которые сами по себе не говорят о сбое. Пример: у нас есть двухфакторная авторизация: спрашиваем пароль пользователя и посылаем СМС. В норме есть процент людей, которые неправильно вводят код. Но если таких людей вместо обычных 5% стало 70%, значит, что-то не в порядке с системой. 

Косвенные (статистические) методы
Сбор жалоб пользователей 

Есть два способа изучения отклонений от сезонности: 
Ручная настройка пороговых значений показателей и их производных. Старый, простой способ детектирования. Например, порогом может быть количество неверно введенных пин-кодов, превышающее 20%.
Машинное обучение. Специальный сервис по историческим данным подбирает модель для вашей сезонности и строит доверительный интервал. Система сигнализирует, если показатель выходит за пределы этого интервала. В такую систему можно выгрузить все метрики. Здесь возможны ошибочные срабатывания из-за того, что не всякое отклонение — признак сбоя. Этот способ хорошо использовать как вспомогательный. Забавный пример: система обучается очень быстро. За время вашей работы над устранением сбоя она уже выучила новое поведение и считает прежнее правильное поведение неправильным. То есть будет сигнализировать о сбое. 

Важный недостаток статистических методов детектирования сбоев — они срабатывают только при достаточном объеме статистики и плохо работают при малом количестве пользователей. Например, трудно отследить ночной сбой, когда количество пользователей падает. На фоне низкой активности статистику может испортить один «странный» пользователь.

Также этот подход не работает, если вы используете важные, но редко используемые функции.

## 13. Что такое алертирование? Что такое пейдж? Что нужно учесть при организации алертирования?
Алертирование — механизм привлечения внимания инженеров.

Что такое пейдж?

Что нужно учесть при организации алертирования?
- Время реакции.
- Обратную связь.
- Кому доставляется алёрт, коллективное безответственность, баззфактор, доставка алёрта только опытным людям.
- Их количество, Алертов должно быть не много
- Приходят только дежурному
- Настойчивая доставка алертов.

## 14. Как оценивается надежность ИТ систем? Что такое SLI/SLO/SLA/бюджет ошибок? В чем разница между TBSLO и RBSLO?
Проблема оценки — невозможность подсчитать, сколько сбоев не произошло благодаря работе SRE.
> Полагаем, что команда SRE работает хорошо, если сбоев не больше определенного количества.

Что такое SLI/SLO/SLA/бюджет ошибок?
Для оценки используется:
- SLI, Service Level Indicators, — метрики, показывающие непосредственный пользовательский опыт. Примеры: время ответа приложения, количество возвращаемых ошибок, количество отказов в обслуживании, время обслуживания (например, размещения объявления на соответствующем ресурсе), качество работы поиска
- SLO, Service Level Objectives, — целевые показатели сервиса, то к чему стремимся.
- SLA, Service Level Agreement, — контракт, то на что закоммителись пользователю. Соглашение между инженерами и работодателем, касающееся поддержки работоспособности сервиса.
- Бюджет ошибок, — то сколько можем позволить себе иметь неработоспособную систему. можно расходовать разными способами. Например, мы принимали решение релизиться почаще, потому что накопился большой бюджет ошибок и сбоев не было. Это редкая история. Если бюджет ошибок отрицательный, значит, нужно реже релизиться и больше тестировать.

## 15. Что такое триггер сбоя? Расскажите про триггеры сбоя?
Триггер — событие, с которого начинается сбой.

> Триггер есть всегда, и он всегда один. При этом не бывает одной корневой причины. Сбой происходит при совпадении многих факторов. 

Триггеры сбоев 
Стихия и внешние техногенные факторы: ураганы, землетрясения, наводнения, пожары, отключение электричества. Минимизировать ущерб может только очень крупная компания ранга Amazon с большим количеством дата-центров в разнообразных географических локациях. 
Релиз. Если баг в часто используемом функционале, то сбой начнется с релиза. Но ошибки могут содержаться в редко используемом функционале. В этом случае релиз не будет триггером сбоя. 
Отказ оборудования. Чем больше инфраструктура, тем чаще происходят отказы. В правильной архитектуре сбоя не будет, а отказ оборудования станет рутинной операцией, незаметной для пользователя. 
Изменения в действиях пользователя. В определенный момент пользователи начинают задействовать функционал более хитрым способом. Или же просто возросла нагрузка со стороны пользователей. Особенно страдают от этого системы, позволяющие делать хитрые запросы. 
Переполнение. Это хитрый триггер. Очевидное переполнение — отсутствие места на диске. Но есть неочевидные: переполнение TCP-портов, превышение лимитов в приложениях, которые установил разработчик, или счетчиков в базах данных. 
Случайное срабатывание бага. Время от времени без особой привязки ваше приложение крашится или зависает. Срабатывает редкое совпадение факторов в приложении, которые сложно обнаружить. 
Изменения в работе внешних систем. У компании есть партнеры, с которыми налажена интеграция. Партнеры могут поменять поведение своих систем намеренно или случайно. У нас в этот момент может наступить сбой. Простой вариант: сломалась внешняя система — и наша тоже прекратила обслуживать пользователей. Более сложный вариант: они добавили новые поля, мы завязали парсер, который строго валидирует поля, и в итоге у нас произошел сбой. 
Действия обученных специалистов, чаще всего ИТ-администраторов. Пример из практики Тинькофф. Была куплена вендорская система хранения данных. Вендор предоставил административный интерфейс. В системе было две кнопки «Удалить». При нажатии на одна удалялось выбранное, на другую — все. Окошки предупреждения для обоих кнопок были одинаковые. Однажды человек перепутал кнопки и машинально удалил все. Был большой сбой. Восстанавливали из бэкапов. 
Действия третьих лиц. Злоумышленники могут вмешаться в работу систем и вызвать сбой. 



Влияние такого триггера, как релиз, можно ослаблять при помощи Code Freeze — делать релизы реже. Действие причин сбоев при этом снижается. Параллельно важно бороться с причинами. Например, с ошибкой программиста — причиной сбоя — можно бороться при помощи QA-инженеров и написания юнит-тестов. 

## 16. Что такое причина сбоя? Расскажите про причины сбоев. Что нужно учитывать при поиске причин сбоя?
Причина — фактор сбоя, на который мы хотим и можем повлиять.

> Триггер есть всегда, и он всегда один. При этом не бывает одной корневой причины. Сбой происходит при совпадении многих факторов. 


Сбою предшествует цепочка событий, каждое из них может быть его причиной. Нас интересуют только те, на которые мы можем повлиять. Именно эти события — причина сбоя. Мы не делаем формальную работу, которая не может повлиять на причину сбоя. 

Причины сбоев 
Ошибка или недоработка в программном коде. Чинится внедрением юнит-тестов, code review, внедрением стандартов.
Ошибка или недоработка в архитектуре приложения. Например, выбрали синхронное приложение, а нужно асинхронное.
Ошибка или недоработка в архитектуре системы (группы приложений). Например, неудачное разделение на микросервисы, неудачно выбранные стораджи. 
Ошибка или недоработка в процессах. Например, могут быть неудачными процессы CI/CD, деплоймента и выкатывания, согласования плановых работ.
Если процесс согласования неудобный, у сотрудника всегда будет желание его обойти. 
Варианты решения: 
упростить процесс согласования;
поднимать важность;
ввести жесткий запрет на действия в системе без согласования. 
Показатели для исследования причин сбоя: 
процесс может быть ошибочен. Старожил может быть в курсе, что в процессе есть сбой, но знает, как его обойти. Новичок не в курсе; 
процесс может подталкивать сотрудника к ошибке; 
технический долг (известно как исправить процесс, но не исправили);  
проблема известна, решение не найдено;
о проблеме узнали впервые. 
Ошибка сотрудника при проведении работ. Один из вариантов — административный интерфейс провоцирует ошибку. 
Ошибка пользователя: отсутствие защиты от некорректного использования. Пользовательский интерфейс иногда может провоцировать ошибку: кнопки расположены неудобно, нет важной информации. 
Проблема в коммуникации. Сотрудник был неверно информирован или не информирован, не прочел информативное сообщение из-за перегрузки информацией. 

Игнорирование риска при желании добиться цели любой ценой. Это причина практически всех крупных техногенных катастроф. На эту тему есть фильм Питера Берга «Глубоководный горизонт» о катастрофе в Мексиканском заливе. Этот же фактор сработал при взрыве Челленджера и в Чернобыле. 
На сотрудника могут давить коллеги или руководитель. Важно следить, не приводит ли давление руководителя к частым сбоям. 
Прямая человеческая проблема: выгорание, депрессии, конфликты с коллегами и, как следствие, нежелание включаться в работу.
Умышленное причинение вреда. Виновниками могут быть сотрудники и третьи лица. Известны случаи, когда обиженные увольнением сотрудники встраивали баги, которые срабатывали после их ухода. 
Недокументированное отклонение в работе внешнего сервиса. Это сложно предсказать. Как правило, настоящую причину мы выяснить не можем. Возможные варианты: атака хакеров на внешний сервис, ошибка разработчика, недоработка в архитектуре. Лучшее, что можно сделать, — сообщать коллегам внутри компании и клиентам, что это ошибка внешнего сервиса. Выход при регулярных сбоях — поменять вендора или разработать сервис самостоятельно. 



Для каждой из них есть ряд стандартных показателей, которые используются для детализации причин сбоев: 
разработан ли код внутри компании; 
взят ли код из открытого проекта;
куплен ли код у сторонней компании; 
не технический долг ли ошибка (проблема была известна ранее, решение найдено, но сбой происходит); 
открытая ли проблема (проблема известна, но решение еще не найдено); 
узнали ли о проблеме впервые. 



Причинами не являются то на что мы не можем повлиять!
- Отказ оборудования иногда указывают в качестве причины сбоя. Но мы не можем считать его причиной, потому что не можем на него воздействовать. Это ведет нас в тупик и не влечет за собой никаких действий. Смена вендора не решит проблему. Чем больше в компании оборудования, тем чаще будут случаться поломки. Наша задача — предусмотреть отказы оборудования и заложить это в архитектуру программного обеспечения. А истинная причина сбоя — в отсутствии защиты от поломок оборудования. 
- Релизы не рассматриваются в качестве причины сбоев. Релиз — нормальная операция в жизненном цикле приложения. При этом релиз и отказ оборудования могут быть триггерами сбоев. 
- Действия пользователей не могут рассматриваться в качестве причины сбоев. Все пользователи, кроме сертифицированных, специально обученных специалистов, могут делать с софтом все что заблагорассудится. Мы должны ожидать от них любого поведения. Если какие-то действия пользователей приводят к сбою, налицо недоработка в софте. 
- Сбой сети — не причина сбоя. Сетевые проблемы — фактор, который постоянно оказывает влияние на работу системы. Чем больше система, тем сильнее это влияние. Причина — в архитектуре, топологии сети, администрировании и обучении наших программ тому, как переживать сетевые проблемы. 
- Внешние атаки. Крупные компании подвергаются атакам постоянно. Мы должны от них защищаться. Причину надо искать том, что мы недостаточно защищены. 
- Человеческий фактор бесполезно указывать в качестве причины сбоев, потому что все важные причины сбоев будут сведены именно к нему. Все люди работают на одном и том же оборудовании, которое имеет примерно одинаковую надежность. Повлиять можно только на то, как мы с этим работаем. Важно выделить разные типы человеческого фактора, чтобы понять, что можно улучшить. 


Существует практика «3 амиго»: заказчик, разработчики и QA-инженеры в тесной коммуникации придумывают, как должен выглядеть функционал. Практика помогает бороться с такой причиной сбоя, как неправильно понятая спецификация. 

Со всеми без исключения причинами сбоев бороться слишком дорого. Убыток от сбоев ниже, чем стоимость предотвращения их всех.

## 17. Что такое настольная книга дежурного? Какая информация должна в нее попадать?
Настольная книга дежурного — общую для всей команды страницу в удобной внутренней системе.
В идеале содержащаяя в сжатом виде минимум полезной информации, которую необходимо знать дежурному.

Сбой — стресс для специалиста с любым опытом. Человек действует медленнее, забывает важные вещи, особенно если их редко применяют.
Поэтому книга призвана это компенсировать, содержа в себе:
- значимые новости по проекту, которые относятся к потенциальному сбою, например что есть экспериментальная версия или свежее изменение функционала (указать алгоритм его выключения);
- моменты увеличения трафика;
- важные для продакшена ссылки, которые имеют отношение к сервису и его починке: дашборды, системы, панели управления;
- контакты со специалистами: разработчиками, руководителями, SRE соседних команд;
- самые полезные, в том числе часто применяемые команды с указанием аргументов командной строки; 
- важные выдержки из документации — советы на случай сбоя. 

## 18. Назовите и опишите RED и USE метрики.
RED — request errors duration, т.е. запросы к приложению с разбиением их по типам.

Сбор статистики позволяет понять, как меняется количество запросов разных типов. Duration — время, которое требуется для обнаружения ошибки. Важна информация, как меняется это время в процессе работы системы. 
RED-метрики важно сохранять:
- Для приходящих мы определяем их количество, число успешных ответов и ответов с ошибкой, длительность обработки.
- Для исходящих фиксируем количество вызовов с разбивкой по типам, количество полученных ответов и ошибок, длительность.

USE — utilization saturation error, т.е. мониторинг исчерпания ресурсов.
Часто к сбоям приводит исчерпание ресурсов: памяти, места на диске, внутренних ресурсов приложения (ограниченное количество воркеров, соединений с базой данных).
USE-метрики нужно собирать для всех исчерпаемых ресурсов. 
Например:
- пул соединений с базой данных.
- утилизацию CPU. 


Метрика насыщения Saturation.. Выстраивается очередь из процессов, которые ждут очереди на выполнение. Метрика Saturation показывает степень нехватки ресурса, например размер очередей. 
Преимущества:
- автоматика срабатывает быстрее человека, особенно во время ночного дежурства;
- высвобождается человеческий ресурс;
- снижается уровень выгорания сотрудников.

В подобных случаях применяются
- авторестарт сервисов, который позволяет решить большой класс проблем в софте. Преимущество: безопасно, его можно доверить машине, (при условии что система имеет множество инстансов и рассчитана на то, что их часть пропадет).
- выключение балансировки: сбойные ноты, сбойные кластеры, сбойные дата-центры, инстансы.  Преимущество: безопасно, дешево, система реагирует на проблему сразу, хороший юзерэкспририенс.
## 19. Что такое самопочинка? Что нужно учитывать при имплементации?
Автохилинг может заниматься автоматическим тюнингом некоторых параметров
Например, в системе есть сет параметров, связанных с сезонностью, которые постоянно надо подкручивать для оптимизации перформанса. Можно придумать систему, которая мониторит параметры и меняет режимы работы баз данных, размер батчей, шардов в соответствии с вашими особенностями. Такая система предотвращает сбои быстрее, чем специалист. 
Издержки:
ошибка в системе может вызвать сбой или усугубить его — этот метод требует двойной осторожности;
если автоматика используется редко, она может незаметно для специалистов выйти из строя. Оптимально, чтобы система работала постоянно, чтобы можно было мониторить ее состояние. 

## 20. Как отрабатываются действия SRE во время сбоев?
SRE-команды должны заранее написать и отработать специальный план восстановления в случае масштабного уничтожения инфраструктуры,
Отработка сбоев существенно уменьшает количество ошибок во время сбоя и ускоряет процесс ликвидации последствий. 
При отработке сбоев применяются следующие практики: 
- Игра «Колесо неудач» — сценарий на основании глубокого анализа прошлых. Олды учат, новички наваливают. Impact: Нарабатывается скорость реакции и кругозор.
- «Теневой дежурный» — Новичок дежурит с опытным коллегой. Обоим одновременно поступает алерт во время реального сбоя. Новичок пытается решить проблему, опытный подстраховывает. 
- Обучение на искусственных сбоях.


## !!! 21. Что такое DRP и DiRT?
Disaster Recovery Plan — DRP. Большая часть сбоев мелкие.

Disaster Recovery Testing — Отработка этого плана — отработать специальный план восстановления в случае масштабного уничтожения инфраструктуры aka целенаправленное обучение команды.

## 22. Как организуется коммуникация во время сбоев? Какие роли следует назначить и что они должны делать?
В идеале прорабатывается заранее.
Организуется War room — реальное или виртуальное пространство (чат) для обмена свежей информацией в случае сбоя. Должно быть технически независимо от основной инфраструктуры, например чат у внешнего провайдера.

Распределение ролей во время сбоя
Ответственный за сбой — координатор ликвидации последствий сбоя.
Коммуникатор — шарит за внутреннюю структуру компании, обладать техническими знаниями.
Сборщик сведений о пострадавших — аккумулирует информацию, которая позже поможет оценить масштабы происшествия.
Ответственный за работу с пострадавшими клиентами решает, что сообщать и что обещать клиентам, как извиняться. 
Ответственный за пиар и контакты со СМИ решает, как реагировать на публикации в прессе. 
Возможно корректировка ролей после сбоев.

## 23. Как мы собираем информацию по сбоям? Какую информацию следует собирать после каждого сбоя?

## 24. Когда и для чего мы пишем постмортемы? Какие разделы должен содержать постмортем?
Постмортемы — анализ, который проводят после того, как произошел сбой.

ЗаЧеМ?:
1. Необходим сбор статистики по множественным сбоям, чтобы понимать, что происходит, по какой причине, что происходит чаще, а что реже. Отсюда можно почерпнуть идеи, как улучшить систему и сделать ее более устойчивой, облегчить восстановление после сбоя.
2. Быстрое восстановление наряду с отсутствием сбоев улучшает пользовательский опыт. 
3. Мы получаем материал для обучения, в первую очередь для новичков. Любой новый член команды может почитать живые истории прошлых сбоев про системы, с которыми будет работать. Там есть детали взаимодействия систем, схемы принятия решений.

Можно сделать дешево с помощью опросника:
Затронул ли сбой весь день или начался в рабочее время? Отделяем дневные сбои от ночных.
Был ли триггером сбоя релиз? Это показывает, насколько хорошо мы пишем код. 
Затронул ли сбой внешних клиентов напрямую или опосредованно, например через сбой в работе колл-центра? 
Был ли сбой вызван проблемами в базовой инфраструктуре: в сети, на «железных» серверах, в дата-центрах, hardware-хранилищах данных? 
Корректно ли сработали мониторинг, алертинг и вовремя ли мы узнали о сбое?
Могли ли мы обнаружить проблему при тестировании? На стадии тестирования могут выявляться, например, ошибки разработчика. Важно знать, какой процент сбоев они дают. 
Знали ли мы о проблеме или баге до того, как она привела к сбою? Вопрос о техническом долге.
Связан ли сбой с ошибкой сотрудника при релизе, плановых работах, обслуживающих продакшен? Так мы увидим защищенность тулов, обученность людей и необходимость доработок. 
Был ли баг в программном коде? Здесь мы отделяем сбои, произошедшие из-за ошибки в программе, от сбоев из-за ошибки в настройках. 
Связан ли сбой с естественным увеличением нагрузки? Естественная нагрузка обычно растет постепенно, но может произойти резкий скачок, например после рекламной кампании или объявления о распродаже. 
Имел ли место форс-мажор? Тут важно понимать, от воздействия каких факторов (пожаров и так далее) мы решили не защищаться. Если форс-мажоры часты, стоит пересмотреть стратегию и строить работу с учетом этих факторов.  
Связан ли сбой с работой партнеров или ошибкой в программном обеспечении вендора? 
Связан ли сбой с открытым программным обеспечением? Анализируя статистику, можно отказываться от такого ПО или дорабатывать его. 
Связан ли сбой с намеренным действием злоумышленников? Это могут быть как внешние, так и внутренние атаки. 
Связан ли сбой с коммуникацией между сотрудниками, например с некорректной постановкой задачи, ошибочным пониманием требований, некачественной документацией и ее отсутствием? 
Если сбой произошел в момент релиза, откатывали ли мы изменения? По моему мнению, в случае проблем с релизом нужно делать откат. Это не всегда возможно, например в случае использования вендорского ПО. 
Применялись ли специальные программные хотфиксы в процессе сбоя? ПО может собираться и релизиться несколько часов. И если в течение этого времени мы применяем много хотфиксов, нужно от этого избавляться или сокращать время релиза. 
Привлекали в процессе устранения сбоя одну SRE-команду или несколько? При привлечении нескольких команд растут требования к качеству взаимодействия. 
Привлекались ли к устранению сбоя разработчики? Необходимость часто привлекать разработчиков — опасная ситуация: они не находятся на дежурстве и в острый момент могут быть недоступны. 

Правила составления постмортема
Постмортем должен быть интересным и содержательным. Это позволит качественнее обучать сотрудников.

Из чего состоит постмортем:
1. Краткое содержание — самая ценная информация для сотрудников, у которых мало времени. По краткому содержанию должно быть ясно, стоит ли читать текст целиком.
Влияние и последствия: сколько денег потеряла компания, в том числе на выплате компенсаций, сколько клиентов затронул сбой, что писали СМИ, какие процессы пришлось перестраивать.
Все причины и триггер сбоя: как он начался, как причины переплетались между собой. Сбой может быть техническим, в этом случае важно указать прошлые ошибки в процессах, которые к нему привели. Сбой может быть организационным. Пример из практики Тинькофф: может быть неверно рассчитано количество представителей, необходимых в данный день для передачи клиентам банковских карт.
Как обнаружен сбой: как работал мониторинг, какие пришли алерты, как об этом узнавали сотрудники.
Какие действия предпринимали для восстановления. Важно указать действия как технических специалистов (рестарты, рекомендации пользователям, применение обходных путей), так и других сотрудников (действия колл-центра, поддержка в чате). 
Информирование о сбое: как предупреждали внешних пользователей, руководителей компании, коллег из других команд, была ли коммуникация удачной.
Удачные решения и действия в процессе сбоя, например хорошо настроенные алерты, качественные средства противодействия. 
Неудачные действия и решения: оставшийся технический долг, плохое тестирование, неудачные инструменты, приводящие к ошибке, недостаточное обучение коллег. На основе этого пункта строятся планы по предотвращению подобных сбоев в будущем.
В чем нам повезло и в чем не повезло. Везение, когда сбой случился ночью, невезение — когда днем. Пример из нашей практики: недавно в момент миграции на одном из устройств скопилась большая очередь, произошел сбой. Миграция в данном случае — невезение.
 Полученные уроки: что нового мы узнали про процессы как в технической части, так и в работе компании.
 План действий: доработки и изменения в архитектуре, коде, дополнительное обучение, смена вендора, подбор открытого ПО, его доработка.
 Хронология: в какой момент какие действия совершались. Процесс сбора хронологии сложен, для его облегчения полезно записывать содержимое war-room-чатов. 



## 25. Как мы защищаемся от ошибок в коде (багов)?

Тестирование. Традиционный способ — юнит-тестирование, его проводят разработчики или QA-инженеры. Делается также интеграционное тестирование. Реже — нагрузочное тестирование и стресс-тестирование

Code review. Можно тратить на эту процедуру больше или меньше времени и управлять количеством багов и проблем в продакшене.

Парное программирование/Swarming'и. Код пишется двумя участниками, и инспекция происходит на этапе проектирования подходов к кодированию.

Безопасно релизится
- Canary/AutoCanary — шушуц нового трафика
- Blue/Green — Параллельно с работой старой версии приложения на части «железа» разворачивается новая версия.
- Red/Black — Параллельно с работой старой версии приложения на части разворачивается новая версия, но трафик переключается только на новую.
- Rolling update
- Feature flag/toggle

При разработке новых версий нужно отказаться от удаления полей — только добавлять новые. Также желательно, чтобы код в любой момент мог работать с данными как старого, так и нового образца. 

Extra.
Разгрести техдолг.
Рефакторинг — дешевле и легче переписать код.
Снижение скорости разработки — метод снижения количества багов. Большое количество сбоев — следствие разнообразных работ и релизов.

## 26. Как можно защититься от ошибок при проведении работ?

Автоматизация. Моторные рутинные действия, при которых человек легко ошибается, лучше автоматизировать. Это освободит время сотрудников. Пример: проводим функциональное и нагрузочное тестирование перед тем, как заводить в сервис пользователей. Сотрудник может забыть провести эти процедуры. Их лучше автоматизировать. Важно предусмотреть «флаги», чтобы во время экстренных работ автоматику отключить. 
Повышение удобства и безопасности интерфейса. Есть пример из практики: на одной странице были использованы две кнопки «Удалить». При нажатии на одну удаляется все полностью, на другую — выделенный фрагмент. Предупреждения выглядели одинаково. Сотрудник перепутал кнопки и вместо выделенного фрагмента удалил весь массив данных. Важно вводить предупреждения, разумные умолчания — искусственные препятствия. Опасные действия должны безопасно подтверждаться. В зависимости от степени опасности можно сделать разного вида предупреждения. Например, при предупреждении опасного действия можно не активировать кнопку «Да» в течение 10 секунд, просить что-то перечислить или ввести фразу. 
Написание детального плана работ. Наличие плана позволяет меньше думать о следующем действии во время работ и реже ошибаться. План должен содержать: 
- перечень дашбордов мониторинга, которые будут открыты;
- какие действия выполняются;
- что нужно проверить после совершения этих действий; 
- с кем коммуницировать, кому отписаться; 
- что делать, если процесс дал сбой, как выключать новый функционал.
Отработка действий. Чем чаще человек совершает действие, тем реже ошибается. Неравномерное распределение работ в команде и появление более опытного сотрудника мешает отработке действий у остальных коллег. Можно отрабатывать действия виртуально или в ходе реальной работы, распределять их между всеми членами команды. Более опытный сотрудник только наблюдает за действиями остальных. 


## 27. Как проблемы в коммуникации могут привести к сбоям? Как можно от этого защититься?
Множество сбоев происходит из-за незнания: человека не оповестили, он не прочел сообщение, пропустил, забыл. Например, человек может пропустить запрет от коллег по поводу действия с некоторыми нодами и вызвать сбой. 

Документацию в идеале нужно встраивать по месту применения. Например, поясняющие фрагменты удобно встраивать в комментарии к коду. Новый сотрудник привыкнет, что в начале конфиг-файла всегда есть комментарий к нему, и всегда будет читать.
Чем короче документация, тем выше вероятность прочтения. Важно давать только самую важную информацию. В частности, SRE-инженеров интересуют действия, которые приводят к сбоям. Надо чистить старую документацию, чтобы она не накапливалась.

Чтение документации хорошо стимулировать экзаменом. Сотрудник, который понимает, что его не допустят в продакшен, изучит документы гораздо внимательнее. Важно опрашивать сотрудника по всем разделам документации, чтобы выявить пробелы в знаниях. 

## 28. Что может переполниться в наших системах? Как от этого можно защититься?
Переполнения — очень опасное явление и частая причина сбоев. Например, приложение может перестать справляться с нарастающим количеством пользователей. В отличие бага во время релиза, который можно откатить, с переполнением очень тяжело справиться. Невозможно попросить пользователей не регистрироваться. 

Что может переполниться:
- Диск.
- Оперативная память.
- ЦПУ.
- Сетевые каналы. Пример: мы собирали логи по сети на одну ноду. В какой-то момент оказалось, что пропускной способности ее канала для этого недостаточно. 
- Файловые дескрипторы. 
- Кэш страниц в Linux. 
- Доступные идентификаторы в базах данных.
- URL достигает предельного размера. 
- Кластеры. Они масштабируются до определенного предела, которого вы не знаете, поскольку не пытались сделать в кластере максимальное число нод. (увеличили ресурсов на инстансах вдвое, сокращение их количества и стали разбираться с балансировкой.)

Как защититься от переполнений:
- Заранее документировать то, что может переполниться. В этом случае ваши коллеги-последователи в компании будут предупреждены о переполнении, которое наступит через несколько лет. 
- Встроить мониторинг идентификаторов для предупреждения переполнения.
- Планировать ресурсы, стресс- и нагрузочное тестирование. Нагрузочное тестирование — увеличение нагрузки. Стресс-тестирование — регистрация максимально возможного количества пользователей. Пример стресс-тестирования: мы рассчитываем, что в системе зарегистрируется 1 млн пользователей. Что будет, если зарегистрируются 10 млн? 
- Встроить возможность временного сброса части трафика. При резком росте числа пользователей лучше часть на время выключить, чем полностью отключить всех клиентов. Например, базы данных при перегрузке умеют сигнализировать, что сейчас работа не может быть выполнена и нужно повторить попытку позже. 
- Встроить защиту — альтернативные алгоритмы работы. Например, можно временно переключаться с новой версии алгоритма, потребляющего относительно много ресурсов, на старую версию, которая потребляет их меньше. Это дает инженерам время придумать, как полноценно масштабироваться. Так умеет делать поиск Google. Он в зависимости от перегруженности применяет все алгоритмы ранжирования или только их часть. Частичное ранжирование считается сбоем, но пользователь все равно получает результат. Graceful degradation (частичная деградация) — способность системы в случае бага предоставлять максимально качественный в данных условиях сервис для клиента. Опциональные возможности сервиса, которые касаются его удобства, важно тестировать и при этом учить сервис работать без них. 
- Уметь работать на закэшированных данных. Обеспечить бесперебойность работы кэша гораздо легче, чем мастер-системы. 
- Встроить возможность работы в режиме чтения. Есть масса ситуаций, когда лучше сохранить для пользователей возможность только читать, чем запретить и читать, и писать. 
- Научиться обслуживать только часть клиентов. Важно управлять этой ситуацией и сообщать отключенным клиентам о сбое, извиняться за неудобства. Отключение 5% клиентов — меньшее зло, чем полномасштабный сбой для всех. 


## 29. Что такое Graceful Degradation? Приведите примеры.

Graceful degradation (частичная деградация) — способность системы в случае бага предоставлять максимально качественный в данных условиях сервис для клиента. Опциональные возможности сервиса, которые касаются его удобства, важно тестировать и при этом учить сервис работать без них. 
Яндекс, Маил Ру, Сервич, микрофронтенды.

## 30. Что надо предусмотреть в приложении, чтобы оно было удобным в обслуживании?
Важная характеристика приложения — время запуска. В процессе развития приложение все больше наращивает время запуска. Это может стать проблемой: во время сбоя нужен оперативный рестарт, а он серьезно задерживается.
Скорость отката релиза.
Приложение должно уметь запускаться без зависимостей. Если зависимости в процессе работы исчезли, приложение должно уметь переподключаться.
Логирование:
- Управлять логированием через API. Мы не хотим рестартовать приложение, менять конфиг, чтобы видеть больше или меньше логов.
- Управлять семплированием логов
- Уметь повышать логирование для конкретного запроса.
Уметь в ретраи

## 31. Что такое повторы? Что нужно учесть при имплементации повторов?
Повторять вызовы в случае неуспешных операций. Повторять можно не все операции. Например, нельзя повторять денежные переводы или покупки. Операции, которые можно повторять, называются идемпотентными. Все внешние API удобно привести к идемпотентному виду. Пример: денежный перевод можно сделать идемпотентным. Если на входе в API есть кэширующий сторедж, операцию можно маркировать. API на входе может проанализировать, был ли повтор операции. Теперь операцию можно повторять с идентификатором. API сама сделает дедуплицирование. 

Ретраи хз кто говорит повторы опасны: если случился сбой и софт обучен повторять все операции, мы сами заддосим все системы из-за резкого возрастания нагрузки. 

Как избежать проблем с повторами: 
Повторы нужно делать с экспоненциальным затуханием. 
В повторы нужно добавлять случайный сдвиг по времени. Пример: сбой в сети, который определяется оборудованием. Сеть строит обходные маршруты, проблемы решаются меньше чем за секунду. За это время может накопиться большой объем операций, претендующих на повторение. В определенный момент образуется большой всплеск запросов, который может перегрузить систему, может возникнуть переполнение, что приведет к большому сбою. Случайные сдвиги повторов по времени распределят нагрузку.
Важно встроить в системы лимиты по количеству запросов в секунду. Приложение отвечает кодом — сообщает о перегрузке. Это защищает от шквала в результате ошибки с повторами. 
Таймауты Ввести глобальный таймаут через несколько систем. В запрос нужно встроить лимит по времени для процессинга.

## 32. Какую опасность несут очереди? Как в коде появляются неявные очереди? Что можно сделать, чтобы защититься от этих опасностей?
Допустим мы проводим нагрузочное тестирование и жоско стреляем сверх нагрузка:
SLA Должны держать 20 000. Мы подаем 200 000 запросов, приложение убито, мы видим ответ с ошибкой, затем возвращаемся на уровень 20 000. Сервис при этом должен уметь восстановиться. 

Очень часто приложение не восстанавливается после пиковой нагрузки. Причина в том, что в приложении есть явные или неявные очереди: процессинг запросов, пулы потоков, фреймворков. 


Способы чистки очереди:
- Создать очередь с фиксированным временем жизни: в момент принятия запроса указать, когда он будет считаться заэкспаренным. 
- Вместо очередей использовать стеки с фиксированным размером. Когда приложение справляется с нагрузкой, размер очередей около нуля. В момент краткого всплеска нагрузки стек позволит отложить часть запросов в буфер. Если нагрузка возрастает, в стеке будут складироваться запросы, а обрабатывать вы будете сначала только самые свежие, поскольку старые запросы не нужны. Вы теряете часть запросов, но время сбоя существенно уменьшается.


## 33. Как организуется работа SRE команд?
Weekly
Важно проводить регулярные продакшен-встречи — раз в неделю или в две недели. Если сложности, то два раза в неделю. На эту встречу разумно приглашать разработчиков и специалистов по продукту. На встрече обсуждаются все сбои, произошедшие за период. 

Важно обсудить:
- Все ли SRE-инженеры в курсе и все ли понимают, что происходило в каждый момент во время сбоя. Важно затронуть даже мелкие происшествия.
- Причины сбоя, задачи и средства противодействия, общее направление развития продукта. Это важно обсудить с разработчиками и со специалистами по продукту.
- Улучшилось ли состояние системы после ликвидации прошлых сбоев. 
- Вести дневник продакшена. Важно записывать все изменения, недоработки. Множество сбоев происходит из-за неосведомленности специалиста, а также во время проведения плановых и внеплановых работ. Если сотрудник возвращается после отпуска, важно ввести его в курс дела. Специалист, выходящий на дежурство, должен прочитать последний лог дневника. После возвращения из отпуска нужно просмотреть записи за время отсутствия. Дневник помогает разобраться со сроками сбоев. 
- Вести документацию на случай сбоев. Во время сбоя сотрудник в стрессе делает больше ошибок. Вещи, которые часто приходится применять, хорошо поместить в отдельный документ. Его важно своевременно обновлять. 


Perf. review/Обучение коллег
Проводить экзамены на знание документации, в том числе перед выходом на дежурство. Проводят его опытные специалисты для новичков.

Дежурства
SRE-команда организует дежурство. Один из членов команды постоянно доступен по телефону.
(Если дежурство специально не организовать, рано или поздно возникнет ситуация, когда ни один из членов команды не доступен. )

Важно наладить коммуникацию с дежурным. В крупной компании нужен грамотный процесс алертирования
- Автоматические алерты должна настраивать SRE-команда.
- Алерты должен получать напрямую дежурный SRE. (Доставлять алерты нужно только звонком: только он гарантированно привлечет внимание дежурного.)
